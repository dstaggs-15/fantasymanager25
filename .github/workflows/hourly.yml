# -----------------------------------------------------------------
# A fresh, rewritten workflow for the Fantasy Football Data Pipeline
# -----------------------------------------------------------------
name: Fantasy Football Data Pipeline

on:
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

  # Runs automatically at 9 minutes past every hour
  schedule:
    - cron: '9 * * * *'

jobs:
  # The main job that will run on your server
  fetch-and-process:
    name: Fetch and Process ESPN Data
    runs-on: [self-hosted, Linux, X64]

    steps:
      # Step 1: Get a copy of your repository's code
      - name: 1. Checkout Code
        uses: actions/checkout@v4

      # Step 2: Set up the Python environment
      - name: 2. Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # Step 3: Cache dependencies to speed up future runs
      - name: 3. Cache Dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      # Step 4: Cache the Playwright browser to save disk space
      - name: 4. Cache Browser
        uses: actions/cache@v4
        with:
          path: /home/ghrunner/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-chromium

      # Step 5: Install the required Python libraries
      - name: 5. Install Python Libraries
        run: pip install -r requirements.txt

      # Step 6: Ensure the Playwright browser is installed
      - name: 6. Install Playwright Browser
        run: python -m playwright install chromium
        env:
          PLAYWRIGHT_BROWSERS_PATH: /home/ghrunner/.cache/ms-playwright

      # Step 7: Run the Python scripts to get and clean the data
      - name: 7. Run Data Pipeline Scripts
        run: |
          echo "--- Starting Step 1: Fetching Raw Data ---"
          python ${{ github.workspace }}/pipeline/fetch_all_with_playwright.py
          echo "--- Starting Step 2: Processing Clean Data ---"
          python ${{ github.workspace }}/pipeline/process_data.py
        env:
          LEAGUE_ID: ${{ vars.LEAGUE_ID }}
          ESPN_USER: ${{ secrets.ESPN_USER }}
          ESPN_PASS: ${{ secrets.ESPN_PASS }}
          # THE FIX: Added the missing SWID and S2 secrets here
          ESPN_SWID: ${{ secrets.ESPN_SWID }}
          ESPN_S2: ${{ secrets.ESPN_S2 }}

      # Step 8: Commit the updated data files back to your repository
      - name: 8. Commit New Data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "ðŸ“Š Chore: Automated fantasy data update"
          file_pattern: docs/data/*.json

      # This step will run ONLY if the job fails, to help with debugging
      - name: Upload Error Screenshot on Failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: error-screenshot
          path: error_screenshot.png
